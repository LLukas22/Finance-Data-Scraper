{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls=[\n",
    "#     \"https://www.cnbc.com/\",\n",
    "#     'http://cnn.com',\n",
    "# ]\n",
    "\n",
    "# for url in urls:\n",
    "#     paper = newspaper.build(url)\n",
    "#     for article in paper.category_urls():\n",
    "#         print(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "import hashlib\n",
    "\n",
    "class RSS_Item(object):\n",
    "    def __init__(self,item:Tag,publisher:str) -> None:\n",
    "        self.publisher = publisher\n",
    "        self.item = item\n",
    "        self.link = item.find('link').text\n",
    "        self.pub_date = item.find('pubDate').text\n",
    "        self.tickers = None\n",
    "        #Seeking Alpha gives us the tickers of the article\n",
    "        if item.find('category'):\n",
    "            categories = item.find_all('category')\n",
    "            self.tickers = [category.text for category in categories]\n",
    "            \n",
    "    @property\n",
    "    def hash(self)->str:\n",
    "        return hashlib.sha512((self.publisher+self.link).encode(\"UTF-8\")).hexdigest()\n",
    "            \n",
    "         \n",
    "publisher = 'seekingalpha'   \n",
    "feed = \"https://seekingalpha.com/market_currents.xml\"\n",
    "# first make a get request to the RSS feed\n",
    "response = requests.get(feed)\n",
    "# collect the contents of the request\n",
    "webpage = response.content\n",
    "# create a BeautifulSoup object that we can then parse to extract the links and title\n",
    "soup = BeautifulSoup(webpage, features='xml')\n",
    "# here we find every instance of an <item> tag, collect everything inside each tag, and store them all in a list\n",
    "items = soup.find_all('item')\n",
    "# extract the article link within each <item> tag and store in a separate list\n",
    "rss_items = []\n",
    "for item in items:\n",
    "    rss_items.append(RSS_Item(item,publisher))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['256cd48b10d576fa4bf952304e2133cfa738c729b20bb64dbf23870c4f3b977438faf93fe4525ba831ac6320b8401143a11435ab07e5dbe61fef383e04ee22f4', '9803e0055ae6bc5ea729bf4b3d8514f3900cfc304c66bdd9735255efdd0c35cbf730a40bcee3b3eb846f94e69d8979bae68696b4f0d7ec420001ddeabc1d1c5a', '44b5fa9ec7f3b19766f62af47a4dc3bdbeb4017a7d6cacf6e9d143f4e904567e6d7bd8317cdc79b9eb16fd21567e5e5c7a39cf6d919fd2ee0379cc8fb2f04db9', 'ade1b6f2ad2e621abe6c0b64b336ee998d4b4c261ea12f832b008a074443c3bbf6482c31a49f51711460511d73ec95f74d238c99ecf637c8deffc5752ea26db1', 'c26e1248a2dfc3c1458f605000873cd8afb67224334e680a872d76d8540af95ab040e9a21f8448071c149c13346d12107cb1ff5ce5a67c7eaa7f163fb1790fb9', '0d9d0f307372245414744f82be348c2a207c6850970076ae423d751ac267164ad1861558d663d05f772f102e0f784e73964533992c786bdb06c4b6d087c37c3a', '0bb7ee02fe9ba940489ed83b002df0f5847e7a9022ec9f8c5634e54fcf64a0ccdb6977792438639a493d858dccd3beba9e47cf26a72072c0c980ea2c612b5083']\n"
     ]
    }
   ],
   "source": [
    "print([rss_item.hash for rss_item in rss_items])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArticleException",
     "evalue": "Article `download()` failed with 403 Client Error: Forbidden for url: https://seekingalpha.com/warnings/3863488-warning-amkby-is-high-risk-of-cutting-dividend?utm_source=feed_news_all&utm_medium=referral on URL https://seekingalpha.com/warnings/3863488-warning-amkby-is-high-risk-of-cutting-dividend?utm_source=feed_news_all&utm_medium=referral",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArticleException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mf:\\Github\\Finance-Data-Scraper\\test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Github/Finance-Data-Scraper/test.ipynb#ch0000004?line=4'>5</a>\u001b[0m article \u001b[39m=\u001b[39m Article(rss_item\u001b[39m.\u001b[39mlink)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Github/Finance-Data-Scraper/test.ipynb#ch0000004?line=5'>6</a>\u001b[0m article\u001b[39m.\u001b[39mdownload()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Github/Finance-Data-Scraper/test.ipynb#ch0000004?line=6'>7</a>\u001b[0m article\u001b[39m.\u001b[39;49mparse()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\newspaper\\article.py:191\u001b[0m, in \u001b[0;36mArticle.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 191\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthrow_if_not_downloaded_verbose()\n\u001b[0;32m    193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mget_parser()\u001b[39m.\u001b[39mfromstring(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhtml)\n\u001b[0;32m    194\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclean_doc \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\newspaper\\article.py:531\u001b[0m, in \u001b[0;36mArticle.throw_if_not_downloaded_verbose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[39mraise\u001b[39;00m ArticleException(\u001b[39m'\u001b[39m\u001b[39mYou must `download()` an article first!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    530\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_state \u001b[39m==\u001b[39m ArticleDownloadState\u001b[39m.\u001b[39mFAILED_RESPONSE:\n\u001b[1;32m--> 531\u001b[0m     \u001b[39mraise\u001b[39;00m ArticleException(\u001b[39m'\u001b[39m\u001b[39mArticle `download()` failed with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m on URL \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    532\u001b[0m           (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_exception_msg, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl))\n",
      "\u001b[1;31mArticleException\u001b[0m: Article `download()` failed with 403 Client Error: Forbidden for url: https://seekingalpha.com/warnings/3863488-warning-amkby-is-high-risk-of-cutting-dividend?utm_source=feed_news_all&utm_medium=referral on URL https://seekingalpha.com/warnings/3863488-warning-amkby-is-high-risk-of-cutting-dividend?utm_source=feed_news_all&utm_medium=referral"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "from newspaper import Article\n",
    "\n",
    "for rss_item in rss_items:\n",
    "    article = Article(rss_item.link)\n",
    "    article.download()\n",
    "    article.parse()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
